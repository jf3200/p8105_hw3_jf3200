---
title: "Homework 3"
author: "Jessica Flynn"
output: github_document
---

```{r setup}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 

```{r load_data}
data("instacart")
```

This data set contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are at the level of items in orders by users, , meaning there is 1 row for each product ordered in each order. There are user /  order variables `user_id`, `order_id`, `order_dow` and `order_hour_of_day`. There are also item variables -- `product_name`, `aisle`, `department`, and numeric codes for `product_id`, `aisle_id` and `department_id`. There are also some variables regarding whether someone re-ordered an item (`reordered`), what number order it is for a user (`order_number`) and how many days since their prior order (`days_since_prior_order`).

How many aisles are there, and which aisles are the most items ordered from?

There are a total for `r instacart %>% count(aisle) %>% nrow()` aisles. The top 10 aisles that the most items are ordered from are output in the table below.

```{r aisles_and_items}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n)) 
```


Plot for aisles with more than 10000+ items ordered. 

```{r add_aisle_plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate( 
    aisle = factor(aisle), 
    aisle = fct_reorder(aisle, n)
    ) %>%
  ggplot(aes(x = aisle, y = n)) + 
  labs(
    title = "Number of Items Ordered in Aisles with 10,000+ Items Ordered",
    x = "Aisle",
    y = "Number of Items Ordered") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  geom_point()
```

Below is a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits".

```{r popular_items}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```


Below is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r mean_hour_items}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour =  mean(order_hour_of_day, na.rm = TRUE)) %>% 
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour) %>% 
  knitr::kable()
```

## Problem 2 

```{r, load_tidy_accel_df}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>%
  pivot_longer(
    cols =  starts_with("activity"),
    names_to = "minute", 
    names_prefix = "activity_",
    values_to = "activity") %>% 
  mutate(weekend = ifelse(day %in% c("Saturday", "Sunday"),1,0), 
         minute = as.numeric(minute), 
         day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))
```

This dataset contains 5 weeks (35 days) worth of accelerometer data from a 63 year old man with a BMI of 25 who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center for congestive heart failure (CHF). The accelerometer measures "activity counts" in one-minute intevals. 

After cleaning, tidying and wrangling the data, the results dataset has `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. Each row of the data set represents one minute for each of the days the accelerometer was utilized. The variables in the dataset include `week` taking values from `r accel_df %>% pull(week) %>% min()` to `r accel_df %>% pull(week) %>% max()` for each week of use, as well as `day_id` ranging from `r accel_df %>% pull(day_id) %>% min()` to `r accel_df %>% pull(day_id) %>% max()` for each day of use. There is also a variable `minute` ranging from `r accel_df %>% pull(minute) %>% min()` to 
`r accel_df %>% pull(minute) %>% max()` for each minute in the day.

Additionally, there is a variable `day` that represents the day of the week the activity was recorded. This variables was converted to a factor in re-ordered to the appropriate days of the week ordering (`r accel_df %>% pull(day) %>% levels()`). A variable `weekend` was created that takes on the value 1 if the `day` is Saturday or Sunday and 0 otherwise. Lastly, the variable `activity` contains a measurement of the activity count for each minute of observation. 


Below is a table of total activity by day for each of the 5 weeks

```{r}
accel_day =
  accel_df %>% 
  group_by(day, week) %>% 
  summarize(total_activity = round(sum(activity, na.rm = TRUE)))

# Make table more readable
accel_day %>% 
    pivot_wider(
    names_from = week, 
    values_from = total_activity) %>%
  knitr::kable()
```

Looking at this table, we notice that this person had very little activity on Saturday during week 4 and week 5 (value of `r min(pull(accel_day,total_activity))`). A general trend that can be noticed is a decrease in activity towards the end of the study, particularly on the weekends. The average total amount of activity on a given day is `r format(mean(pull(accel_day,total_activity)),scientific=FALSE)` and it ranged from `r min(pull(accel_day,total_activity))` to `r format(max(pull(accel_day,total_activity)), scientific=FALSE)`. 

ANY TREND I AM MISSING?

 
Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph. 
##NEED HELP WITH THIS

```{r}
accel_df %>% 
  group_by(day_id) %>%
  ggplot(aes(x = minute, y = activity, color = day)) + 
  geom_line()
```




## Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```
change tmin/tmax from 10ths of a degree to a degree

part 2 - group by and summarize + ggplot


part 3 - 2 different plot and merge them using patchwork